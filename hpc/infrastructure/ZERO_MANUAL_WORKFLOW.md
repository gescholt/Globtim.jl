# Zero-Manual Job ID Workflow

## ğŸ¯ **The Problem Solved**

**Before**: Create job â†’ Submit â†’ Copy job ID â†’ Monitor â†’ Copy computation ID â†’ Pull results  
**After**: One command â†’ Automatic everything â†’ Results appear locally

## ğŸš€ **Complete Automated Solutions**

### **Solution 1: Fully Automated Submit-and-Track**

**One command does everything:**
```bash
# Submit job and automatically pull results when complete
./hpc/infrastructure/submit_and_track.sh

# With custom parameters
./hpc/infrastructure/submit_and_track.sh -t thorough -d 10 -b legendre --description "Paper analysis"

# Quick test
./hpc/infrastructure/submit_and_track.sh -t quick -d 6
```

**What this does automatically:**
1. âœ… Creates JSON-tracked job with your parameters
2. âœ… Submits to HPC cluster  
3. âœ… Tracks job ID and computation ID automatically
4. âœ… Monitors job progress every 5 minutes
5. âœ… Automatically pulls results when complete
6. âœ… Notifies you when results are ready
7. âœ… Shows you exactly where to find your results

**No manual job ID copying ever needed!**

### **Solution 2: Background Auto-Pull Daemon**

**Set up once, works forever:**
```bash
# Add to your crontab (runs every 5 minutes)
crontab -e
# Add this line:
*/5 * * * * cd /path/to/globtim && ./hpc/infrastructure/auto_pull_daemon.sh >/dev/null 2>&1

# Or run as background daemon
nohup bash -c 'while true; do ./hpc/infrastructure/auto_pull_daemon.sh; sleep 300; done' &
```

**What this does:**
- âœ… Runs in background checking for completed jobs
- âœ… Automatically pulls any completed results
- âœ… Logs all activity for review
- âœ… Handles multiple jobs simultaneously
- âœ… Never interferes with running jobs

### **Solution 3: Manual Auto-Pull (When Needed)**

**For occasional use:**
```bash
# Check and pull any completed jobs
python3 hpc/infrastructure/job_tracker.py --auto-pull

# List all tracked jobs
python3 hpc/infrastructure/job_tracker.py --list

# Clean up old job records
python3 hpc/infrastructure/job_tracker.py --cleanup 30
```

## ğŸ“Š **Complete Workflow Examples**

### **Example 1: Your Deuflhard Analysis (Fully Automated)**

```bash
# Single command - everything automated
./hpc/infrastructure/submit_and_track.sh -t standard -d 8 -b chebyshev --description "Deuflhard degree 8 analysis"
```

**Output you'll see:**
```
=== Enhanced Job Submission with Auto-Tracking ===
ğŸ“‹ Step 1: Creating JSON-tracked job...
âœ… Job created successfully
Computation ID: abc12345

ğŸš€ Step 2: Submitting to HPC cluster...
âœ… Job submitted successfully
Job ID: 59772335

ğŸ“Š Step 3: Adding to job tracker...
âœ… Job added to tracker

ğŸ¤– Starting automatic monitoring and result pulling...
[14:30:15] Check #1 - Monitoring job 59772335...
  â³ Job still running, will check again in 300 seconds...

[14:35:20] Check #2 - Monitoring job 59772335...
  â³ Job still running, will check again in 300 seconds...

[14:40:25] Check #3 - Monitoring job 59772335...
ğŸ‰ SUCCESS! Results automatically pulled for computation abc12345

ğŸ“ Your results are available at:
  By date: hpc/results/by_date/2025-08-05/abc12345
  By function: hpc/results/by_function/Deuflhard/

ğŸ”¬ Next steps:
  1. Explore the results: ls hpc/results/by_date/2025-08-05/abc12345
  2. Load into notebook: JSON3.read(read("path/to/output_results.json", String), Dict)
```

### **Example 2: Parameter Sweep (Multiple Jobs)**

```bash
# Submit multiple jobs - all tracked automatically
./hpc/infrastructure/submit_and_track.sh -t standard -d 4 -b chebyshev --submit-only --description "Degree sweep 4"
./hpc/infrastructure/submit_and_track.sh -t standard -d 6 -b chebyshev --submit-only --description "Degree sweep 6"  
./hpc/infrastructure/submit_and_track.sh -t standard -d 8 -b chebyshev --submit-only --description "Degree sweep 8"

# Set up background auto-pull to handle all of them
./hpc/infrastructure/auto_pull_daemon.sh &

# Check progress anytime
python3 hpc/infrastructure/job_tracker.py --list
```

### **Example 3: Set-and-Forget Background Mode**

```bash
# Set up once - handles all future jobs automatically
crontab -e
# Add: */5 * * * * cd /path/to/globtim && ./hpc/infrastructure/auto_pull_daemon.sh >/dev/null 2>&1

# Now just submit jobs normally - results appear automatically
./hpc/infrastructure/submit_and_track.sh --submit-only
./hpc/infrastructure/submit_and_track.sh -t quick --submit-only

# Results automatically pulled to hpc/results/ when complete
```

## ğŸ”§ **System Components**

### **Job Tracker (`job_tracker.py`)**
- Maintains database of submitted jobs
- Tracks job IDs, computation IDs, and status
- Automatically checks SLURM for completion
- Pulls results when jobs complete
- Handles failures and retries

### **Enhanced Submission (`submit_and_track.sh`)**
- Creates JSON-tracked jobs
- Submits to cluster automatically
- Adds to job tracker
- Optional real-time monitoring
- Automatic result pulling

### **Auto-Pull Daemon (`auto_pull_daemon.sh`)**
- Background service for result pulling
- Cron-friendly for scheduled execution
- Logging and error handling
- Prevents duplicate instances

## ğŸ“ **Job Tracking Database**

The system maintains a simple JSON database at `hpc/infrastructure/.job_tracker.json`:

```json
{
  "jobs": {
    "abc12345": {
      "job_id": "59772335",
      "computation_id": "abc12345", 
      "status": "COMPLETED",
      "pulled": true,
      "submitted_at": "2025-08-05T14:30:00",
      "pulled_at": "2025-08-05T14:45:00",
      "description": "Deuflhard degree 8 analysis"
    }
  }
}
```

## ğŸ” **Monitoring and Status**

### **Check Job Status**
```bash
# List all tracked jobs
python3 hpc/infrastructure/job_tracker.py --list

# Check for completed jobs
python3 hpc/infrastructure/job_tracker.py --auto-pull

# View auto-pull log
tail -f hpc/infrastructure/.auto_pull.log
```

### **Manual Intervention (If Needed)**
```bash
# Force pull specific computation
./hpc/infrastructure/pull_results.sh --computation-id abc12345

# Check SLURM status directly
python hpc/monitoring/python/slurm_monitor.py --analyze 59772335

# Clean up old tracking records
python3 hpc/infrastructure/job_tracker.py --cleanup 30
```

## ğŸ¯ **Recommended Workflows**

### **For Daily Use**
```bash
# Set up background auto-pull once
crontab -e
# Add: */5 * * * * cd /path/to/globtim && ./hpc/infrastructure/auto_pull_daemon.sh >/dev/null 2>&1

# Then just submit jobs - results appear automatically
./hpc/infrastructure/submit_and_track.sh --submit-only
```

### **For Interactive Work**
```bash
# Submit with real-time monitoring
./hpc/infrastructure/submit_and_track.sh -t quick -d 6

# Watch it run and automatically get results
```

### **For Parameter Sweeps**
```bash
# Submit multiple jobs
for deg in 4 6 8 10; do
    ./hpc/infrastructure/submit_and_track.sh -t standard -d $deg --submit-only --description "sweep_deg_$deg"
done

# Background daemon handles all results automatically
```

## ğŸš¨ **Error Handling**

The system handles common issues automatically:

- **SSH connection failures**: Retries with exponential backoff
- **Job failures**: Marks as failed, doesn't retry pulling
- **Partial results**: Attempts to pull what's available
- **Network issues**: Logs errors and continues monitoring
- **Duplicate pulls**: Skips already-pulled results

## ğŸ’¡ **Pro Tips**

1. **Set up background auto-pull** for hands-off operation
2. **Use `--submit-only`** for batch job submission
3. **Check logs** if something seems wrong: `tail -f hpc/infrastructure/.auto_pull.log`
4. **Clean up periodically**: `python3 hpc/infrastructure/job_tracker.py --cleanup 30`
5. **Use descriptive descriptions** to track your work

## ğŸ‰ **The Result**

**Your new workflow:**
1. `./hpc/infrastructure/submit_and_track.sh` (one command)
2. â˜• Get coffee
3. Results appear in `hpc/results/` automatically
4. Load into notebook and analyze

**No more:**
- âŒ Copying job IDs
- âŒ Remembering computation IDs  
- âŒ Manual result pulling
- âŒ Checking job status repeatedly
- âŒ Losing track of running jobs

**Just pure science! ğŸš€**
