# Machine Learning Integration Milestone
## Neural-Guided Adaptive Optimization for Polynomial Approximation

**Status**: üìã Planned - Research & Formalization Phase
**Target**: Q2-Q3 2025
**Priority**: Medium-High (Research Track)
**Dependencies**: Parameter Tracking Infrastructure ‚úÖ, HPC Deployment, SciML ecosystem (DiffEqFlux.jl, Lux.jl)
**GitLab Milestones**: #6, #7, #8, #9 (see below)

---

## üéØ Vision

Integrate machine learning (neural networks, reinforcement learning) with GlobTim's polynomial approximation and optimization methods to create **adaptive decision-making systems** that learn optimal refinement strategies, degree selection, and domain decomposition policies.

### Core Concept

Instead of using fixed heuristics for:
- When to subdivide domains
- When to increase polynomial degree
- When to drop unpromising subdomains
- How to balance exploration vs exploitation

...train ML agents that learn these decisions through interaction with the approximation/optimization process, using reward signals based on convergence metrics, trajectory quality, and computational efficiency.

---

## üî¨ Research Directions

### Direction 1: RL-Guided Adaptive Mesh Refinement (AMR)

**Problem Statement**: Learn optimal adaptive refinement policies for polynomial approximation domains.

**Inspiration**: Recent work (2024) on deep RL for AMR in finite element methods:
- Formulate AMR as a Markov Decision Process
- Multi-agent RL where each subdomain is an agent
- Local reward formulation based on error reduction
- No exact solution required for training

**GlobTim Application**:
- **State space**: Current approximation quality (L2 error), polynomial degree, subdomain characteristics, critical point distribution
- **Action space**: {subdivide, increase degree, maintain, drop subdomain}
- **Reward function**: Reduction in approximation error, computational cost, critical point discovery rate
- **Policy network**: Maps subdomain features ‚Üí refinement action

**Key Advantage**: Learn domain-specific refinement strategies from experience rather than using fixed heuristics.

---

### Direction 2: Neural Network Controllers for Dynamical Systems

**Problem Statement**: Train neural network controllers to find multiple trajectories reaching target states in controlled dynamical systems.

**Integration with GlobTim**:
1. **Problem Setup**:
   - Dynamical system: `dx/dt = f(x, u)` where `u` is control input
   - Target: Reach target state `x_target` from various initial conditions
   - Use GlobTim polynomial approximation for system modeling

2. **Neural Control Architecture**:
   - Policy network: `œÄ(x) ‚Üí u` (state ‚Üí control)
   - Value network: Estimates trajectory quality
   - Trained with PPO, SAC, or TD3 (continuous control RL algorithms)

3. **GlobTim's Role**:
   - Polynomial approximation of dynamics for fast forward simulation
   - Critical point analysis to identify equilibria and stability regions
   - Level set visualization of reachable sets

4. **Multi-Trajectory Discovery**:
   - Reward structure encourages finding diverse trajectories
   - Different initial conditions ‚Üí different neural controllers
   - GlobTim analysis of trajectory manifolds

**Example System**: Lotka-Volterra 4D model with control inputs:
```julia
# Controlled LV system
dx‚ÇÅ/dt = Œ±‚ÇÅx‚ÇÅ - Œ≤‚ÇÅx‚ÇÅx‚ÇÇ + u‚ÇÅ
dx‚ÇÇ/dt = -Œ≥‚ÇÅx‚ÇÇ + Œ¥‚ÇÅx‚ÇÅx‚ÇÇ + u‚ÇÇ
dx‚ÇÉ/dt = Œ±‚ÇÇx‚ÇÉ - Œ≤‚ÇÇx‚ÇÉx‚ÇÑ + u‚ÇÉ
dx‚ÇÑ/dt = -Œ≥‚ÇÇx‚ÇÑ + Œ¥‚ÇÇx‚ÇÉx‚ÇÑ + u‚ÇÑ
```

**Reward Design**:
- Primary: Distance reduction to target: `r_target = -||x - x_target||¬≤`
- Penalty: Control effort: `r_control = -Œª||u||¬≤`
- Bonus: Reaching proximity threshold: `r_proximity = +100` if `||x - x_target|| < Œµ`
- Diversity: Encourage different trajectory families

---

### Direction 3: Meta-Learning for Parameter Estimation

**Problem Statement**: Learn to quickly adapt approximation strategies for new parameter estimation problems.

**Concept**: Train a meta-learner that, given a new objective function:
- Predicts good initial polynomial degrees
- Suggests domain decomposition strategy
- Estimates required computational budget

**Two-Level Learning**:
1. **Meta-level**: Learn across problem families (different ODEs, different dimensions)
2. **Task-level**: Fast adaptation to specific problem instance

**GlobTim Integration**:
- Use existing parameter tracking infrastructure to build dataset
- Features: Function smoothness, domain size, target accuracy
- Outputs: Recommended GlobTim configuration

**Julia Implementation**: Use Lux.jl for explicit parameter handling (better for meta-learning)

---

### Direction 4: Neural-Guided Degree Selection

**Problem Statement**: Learn optimal polynomial degree progression strategy.

**Current Approach**: Fixed degree or manual tuning
**ML Approach**: Contextual bandit or RL agent that observes approximation progress and decides degree updates

**State Features**:
- Current L2 approximation error
- Polynomial sparsity patterns
- Critical point detection success rate
- Computational cost trends
- Hessian condition number statistics

**Actions**:
- Increase degree by 1, 2, 5, or 10
- Decrease degree (sparsify more aggressively)
- Maintain current degree
- Switch basis type (Chebyshev ‚Üî Legendre)

**Reward**:
```julia
reward = -Œ± * L2_error - Œ≤ * computation_time + Œ≥ * critical_points_found
```

**Training Strategy**:
- Offline: Train on historical experiment data
- Online: Continue learning during actual experiments (safe exploration)

---

## üõ†Ô∏è Technical Architecture

### Core Components

#### 1. ML-GlobTim Interface Layer
```julia
module MLGlobTim

export RLEnvironment, StateSpace, ActionSpace, RewardFunction

"""
Abstract interface for RL environments wrapping GlobTim operations
"""
abstract type GlobTimRLEnv end

struct DomainRefinementEnv <: GlobTimRLEnv
    problem::ExperimentConfig
    state_extractor::Function
    action_handler::Function
    reward_computer::Function
end

function step!(env::GlobTimRLEnv, action)
    # Execute action in GlobTim
    # Compute new state
    # Calculate reward
    return (state=new_state, reward=r, done=done, info=info)
end

end
```

#### 2. Training Infrastructure
```julia
module TrainingInfra

using Flux  # or Lux
using ReinforcementLearning
using DifferentialEquations  # for Neural ODEs

export train_policy, evaluate_policy, save_model, load_model

"""
Train RL policy for adaptive refinement
"""
function train_policy(
    env::GlobTimRLEnv;
    algorithm=:PPO,
    n_episodes=1000,
    checkpoint_freq=100
)
    # Setup policy network
    policy = Chain(
        Dense(state_dim => 128, relu),
        Dense(128 => 128, relu),
        Dense(128 => action_dim)
    )

    # Training loop with GlobTim experiments
    # ...

    return trained_policy
end

end
```

#### 3. Evaluation & Analysis
```julia
module MLAnalysis

export compare_strategies, visualize_learning_curves, analyze_decisions

"""
Compare ML-guided vs traditional heuristics
"""
function compare_strategies(
    test_problems::Vector{ExperimentConfig},
    ml_policy::Policy,
    baseline_heuristic::Function
)
    results = DataFrame()

    for problem in test_problems
        # Run with ML policy
        ml_result = run_experiment(problem, strategy=ml_policy)

        # Run with baseline
        baseline_result = run_experiment(problem, strategy=baseline_heuristic)

        push!(results, compare_metrics(ml_result, baseline_result))
    end

    return results
end

end
```

---

## üß™ Concrete Toy Examples

### Example 1: 2D Rosenbrock with RL-Guided Subdivision

**Setup**:
- Function: `f(x,y) = (1-x)¬≤ + 100(y-x¬≤)¬≤`
- Domain: `[-2, 2] √ó [-1, 3]`
- Goal: Approximate well near minimum with minimal subdomains

**RL Agent**:
- **State**: Current subdomain error map (8√ó8 grid of local L2 errors)
- **Action**: Which subdomain to refine next (64 discrete actions)
- **Reward**: `r = -error_reduction / computational_cost`

**Success Metric**: Achieve target accuracy with 30% fewer subdomains than uniform refinement

**Expected Behavior**: Agent learns to focus refinement near valley and minimum, ignore flat regions

---

### Example 2: Controlled 2D Oscillator - Multi-Trajectory NN Controller

**Dynamical System**:
```julia
# Damped oscillator with control
dx/dt = v
dv/dt = -œâ¬≤x - 2Œ∂œâv + u

# Parameters: œâ=2œÄ, Œ∂=0.1 (underdamped)
```

**Task**: From 10 different initial conditions, reach target `(x=0, v=0)` via control `u`

**Neural Controller**:
```julia
using Flux

controller = Chain(
    Dense(2 => 32, tanh),
    Dense(32 => 32, tanh),
    Dense(32 => 1, tanh),
    x -> x .* 10.0  # Scale control output
)
```

**Training**:
- Algorithm: Soft Actor-Critic (SAC) for continuous control
- Episodes: 1000 training episodes
- Reward: `-||x||¬≤ - 0.1*u¬≤ + 100*(done && ||x|| < 0.1)`

**GlobTim Role**:
- Polynomial approximation of learned controller: `u ‚âà p(x, v)`
- Analyze stability regions via critical points
- Generate phase portraits with optimal trajectories

**Success Metric**: All 10 initial conditions converge within 100 time steps with smooth control signals

---

### Example 3: Degree Adaptation for 3D Peaks Function

**Function**:
```julia
peaks_3d(x, y, z) =
    3*(1-x)^2 * exp(-(x^2) - (y+1)^2 - z^2)
    - 10*(x/5 - x^3 - y^5) * exp(-x^2 - y^2 - z^2)
    - 1/3*exp(-(x+1)^2 - y^2 - z^2)
```

**ML Task**: Learn degree adaptation policy

**State Vector** (dim=15):
- Current degree (3 values: one per dimension)
- L2 error (last 3 iterations)
- Polynomial coefficient sparsity (3 values)
- Critical points found (scalar)
- Computation time ratio (current/budget)
- Hessian condition number (mean, max)

**Actions** (dim=4):
- Increase all degrees +1
- Increase worst dimension +2
- Maintain current
- Sparsify (decrease effective degree)

**Training Data**:
- Run 500 approximations with random degree sequences
- Label each step with future L2 error improvement
- Train supervised policy via imitation learning
- Fine-tune with policy gradient RL

**Success Metric**: Reach target L2 error = 10‚Åª‚Å∂ in 40% less time than fixed degree schedule

---

## üìä Experimental Design

### Phase 1: Foundation (Weeks 1-4)

**Objectives**:
- Implement basic RL environment wrapper for GlobTim
- Create simple state/action/reward infrastructure
- Establish baseline performance metrics

**Deliverables**:
1. `MLGlobTim.jl` module with environment interface
2. Simple example: 1D function with degree selection agent
3. Comparison framework: ML vs fixed heuristics

**Tools**: Flux.jl, ReinforcementLearning.jl

---

### Phase 2: 2D Toy Problems (Weeks 5-8)

**Objectives**:
- Implement Direction 1 (AMR with RL) on 2D problems
- Implement Direction 4 (degree selection) on 2D problems
- Validate learning convergence and performance gains

**Test Problems**:
1. Rosenbrock function (narrow valley)
2. Ackley function (many local minima)
3. Rastrigin function (highly multimodal)

**Experiments**:
- Train 3 separate agents (one per problem)
- Evaluate on held-out initial conditions
- Measure: total cost, final accuracy, decisions made

**Success Criteria**:
- At least 20% improvement over baseline on 2/3 problems
- Learned policies show interpretable patterns

---

### Phase 3: Controlled Dynamical Systems (Weeks 9-12)

**Objectives**:
- Implement Direction 2 (neural controllers)
- Integrate with DifferentialEquations.jl and DiffEqFlux.jl
- Demonstrate multi-trajectory discovery

**Systems**:
1. 2D damped oscillator (as Example 2)
2. Van der Pol oscillator with control
3. 4D Lotka-Volterra with control (if time permits)

**Experiments**:
- Train PPO/SAC controllers for each system
- Extract polynomial approximations of learned controllers
- Use GlobTim to analyze controller landscape

**Success Criteria**:
- Controllers successfully reach targets from ‚â•80% of initial conditions
- Polynomial approximations retain ‚â•90% controller performance
- Visualizations reveal structure of optimal control strategies

---

### Phase 4: Meta-Learning & Generalization (Weeks 13-16)

**Objectives**:
- Implement Direction 3 (meta-learning for new problems)
- Test generalization across problem families
- Develop practical deployment guidelines

**Approach**:
- Create dataset of 100+ GlobTim experiments (varied functions/dimensions)
- Train meta-learner to predict good configurations
- Test on completely new problem types

**Meta-Learning Algorithm**: MAML (Model-Agnostic Meta-Learning) or Reptile

**Success Criteria**:
- Meta-learner achieves 30% faster convergence on new problems (vs default config)
- Generalization across 2D‚Üí3D and smooth‚Üínonsmooth functions

---

## üéØ Success Metrics & Evaluation

### Primary Metrics

1. **Computational Efficiency**
   - Time to reach target accuracy (vs baseline)
   - Total function evaluations required
   - Polynomial operations count

2. **Solution Quality**
   - Final L2 approximation error
   - Critical points detection rate
   - Stability of learned policies

3. **Generalization**
   - Performance on held-out test problems
   - Transfer across problem dimensions
   - Robustness to parameter variations

### Secondary Metrics

4. **Interpretability**
   - Can we understand what the agent learned?
   - Do learned strategies align with expert intuition?
   - Visualization of decision boundaries

5. **Practical Deployment**
   - Training time requirements
   - Model size and inference speed
   - Integration complexity with existing workflows

---

## üîó Integration with Existing Infrastructure

### Leverage Existing GlobTim Components

1. **Parameter Tracking** (Issue #124 ‚úÖ):
   - Use JSON config system for experiment specification
   - ML agents read/write GlobTim configs
   - Automatic logging of ML decisions

2. **HPC Deployment**:
   - Train RL agents on HPC cluster
   - Distributed training across multiple experiments
   - Use existing job submission infrastructure

3. **Visualization**:
   - Extend dashboard to show ML agent decisions
   - Visualize learning curves alongside approximation quality
   - Interactive exploration of learned policies

4. **Post-Processing**:
   - Integrate ML metrics into analysis pipeline
   - Compare ML vs traditional strategies in reports
   - Statistical significance testing

---

## üìö Technical Stack

### Julia SciML Ecosystem

The **SciML (Scientific Machine Learning)** ecosystem provides world-class tools for combining machine learning with scientific computing:

| Component | Package | Role |
|-----------|---------|------|
| **Core SciML** | | |
| Neural Networks | **Lux.jl** | Explicit parameterization (preferred over Flux) |
| Neural ODEs | **DiffEqFlux.jl** | Pre-built implicit layers, O(1) backprop |
| ODE Solving | **DifferentialEquations.jl** | High-performance DE solvers (ODEs, SDEs, DDEs, DAEs) |
| Symbolic Modeling | **ModelingToolkit.jl** | Acausal symbolic-numeric modeling |
| Physics-Informed ML | **NeuralPDE.jl** | PINNs for solving PDEs |
| **ML & RL** | | |
| RL Algorithms | **ReinforcementLearning.jl** | PPO, SAC, DQN implementations |
| Optimization | **Optimization.jl / Optim.jl** | Training optimizers |
| Automatic Diff | **ForwardDiff.jl / Zygote.jl** | Forward & reverse mode AD |
| **Existing GlobTim** | | |
| Data Handling | **DataFrames.jl** ‚úÖ | Experiment data |
| Visualization | **Makie.jl** ‚úÖ | Learning curves, trajectories |

### Why SciML?

- **Universal Differential Equations**: Embed neural networks into DE systems with physical constraints
- **Differentiable everything**: Entire pipeline from neural networks ‚Üí DE solvers ‚Üí loss functions
- **GPU acceleration**: Automatic GPU support when initial conditions are GPU arrays
- **Performance**: Julia's speed + composable architecture
- **Scientific computing**: Purpose-built for physics-informed ML, not just generic deep learning
- **Active development**: Major update in 2024-2025 (DiffEqFlux moved from Flux to Lux)

### Key SciML Capabilities for GlobTim

1. **Neural ODEs**: Learn dynamics from data, perfect for controlled dynamical systems (Direction 2)
2. **Universal ODEs**: Mix known physics with learned components
3. **Adjoint methods**: O(1) memory backpropagation through ODE solves
4. **Stiff/non-stiff solvers**: Handle challenging systems (e.g., stiff Lotka-Volterra)
5. **GPU support**: Scale to large experiments
6. **Symbolic preprocessing**: ModelingToolkit optimizes computational graphs automatically

---

## üöß Challenges & Mitigation

### Challenge 1: Reward Function Design

**Problem**: Defining rewards that lead to desired behaviors is non-trivial

**Mitigation**:
- Start with simple, interpretable rewards
- Use reward shaping based on domain knowledge
- Implement reward debugging tools
- Consider inverse RL (learn rewards from expert demonstrations)

### Challenge 2: Training Data Requirements

**Problem**: RL often requires many episodes to learn

**Mitigation**:
- Use GlobTim's fast polynomial evaluation for cheap simulation
- Leverage existing experiment database for offline RL
- Implement curriculum learning (start with easy problems)
- Consider imitation learning to bootstrap from heuristics

### Challenge 3: Generalization Gaps

**Problem**: Agents may overfit to training problem distribution

**Mitigation**:
- Diverse training set (vary dimensions, smoothness, domains)
- Regularization techniques (dropout, weight decay)
- Domain randomization during training
- Extensive held-out test set evaluation

### Challenge 4: Integration Complexity

**Problem**: Adding ML components increases system complexity

**Mitigation**:
- Clear module boundaries (MLGlobTim interface)
- Fallback to traditional heuristics if ML fails
- Extensive testing and validation
- Gradual rollout (opt-in for users)

### Challenge 5: Computational Cost

**Problem**: Training neural networks requires significant compute

**Mitigation**:
- Leverage HPC cluster for training
- Efficient environment implementations (vectorization)
- Pre-train on simpler problems, fine-tune on complex ones
- Cache and reuse trained models

---

## üìñ Literature & Prior Art

### Key References

1. **Deep RL for AMR**:
   - Luca et al. (2023) "Deep Reinforcement Learning for Adaptive Mesh Refinement", Journal of Computational Physics
   - Karumuri et al. (2024) "G-Adaptive mesh refinement leveraging GNNs"

2. **Neural ODEs for Control**:
   - Chen et al. (2018) "Neural Ordinary Differential Equations"
   - Shen (2020) "Neural ODE for RL and Optimal Control"

3. **Meta-Learning**:
   - Finn et al. (2017) "Model-Agnostic Meta-Learning (MAML)"
   - Nichol et al. (2018) "Reptile: A Scalable Meta-Learning Algorithm"

4. **Julia ML Ecosystem**:
   - Flux.jl documentation (2025)
   - DiffEqFlux.jl examples
   - ReinforcementLearning.jl tutorials

### Related Work in Numerical Methods

- Learned solvers for PDEs (e.g., FEniCS + ML)
- Neural network surrogates for expensive simulations
- Active learning for adaptive sampling
- Bayesian optimization for hyperparameter tuning

---

## üéì Educational Value

This milestone offers rich learning opportunities:

1. **For Students**: Hands-on experience with cutting-edge ML+scientific computing
2. **For Research**: Novel application domain for RL (polynomial approximation)
3. **For Community**: Open-source Julia ML examples in computational mathematics
4. **For Publications**: Multiple paper opportunities (methods, applications, software)

---

## üõ£Ô∏è Roadmap Alignment

This milestone complements existing GlobTim roadmap:

- **Builds on**: Parameter Tracking ‚úÖ, HPC Deployment, Visualization
- **Enables**: Next-gen adaptive algorithms, automated tuning, user-friendly defaults
- **Synergies**: Can accelerate other milestones by learning good configurations

**Strategic Fit**: Positions GlobTim at intersection of classical numerical methods and modern ML/AI

---

## üìù Next Steps

### Immediate Actions (Month 1)

1. **Literature Review**: Deep dive into RL for AMR and neural control papers
2. **Prototype Environment**: Simple 1D RL wrapper for GlobTim degree selection
3. **Baseline Experiments**: Establish performance of current heuristics on diverse problems
4. **Package Evaluation**: Test Flux.jl, Lux.jl, ReinforcementLearning.jl on simple examples

### Short-term (Months 2-3)

5. **Implement Direction 4**: Degree selection agent with PPO
6. **2D Test Cases**: Rosenbrock, Ackley with learned refinement
7. **Documentation**: Write tutorial for ML-GlobTim interface
8. **Visualization Tools**: Add learning curve and decision plotting

### Medium-term (Months 4-6)

9. **Direction 2 Implementation**: Neural controllers for 2D oscillator
10. **Integration Testing**: ML agents in full GlobTim pipeline
11. **HPC Deployment**: Train agents on falcon cluster
12. **First Results Paper**: Draft publication on ML-guided polynomial approximation

---

## ü§î Open Questions

1. **Reward Design**: What's the best balance between accuracy, speed, and interpretability in rewards?

2. **State Representation**: Should we use raw polynomial coefficients, derived features, or learned embeddings?

3. **Action Granularity**: Discrete actions (subdivide/not) or continuous (degree as real number)?

4. **Online vs Offline**: Train purely offline on historical data, or enable online learning during experiments?

5. **Multi-Agent Coordination**: For domain decomposition, how should subdomain agents communicate?

6. **Transfer Learning**: Can an agent trained on 2D problems work on 3D? How to encourage this?

7. **Uncertainty Quantification**: Should agents be Bayesian (output uncertainty over actions)?

8. **Human-in-the-Loop**: How to allow user guidance/correction of agent decisions?

---

## üìä Preliminary Evaluation Plan

### Benchmark Suite

Create standardized test suite:

| Category | Functions | Dimensions | Difficulty |
|----------|-----------|------------|------------|
| Smooth | Polynomials, trig | 1D-3D | Easy |
| Narrow Features | Rosenbrock, ridge | 2D-4D | Medium |
| Multimodal | Ackley, Rastrigin | 2D-3D | Hard |
| Discontinuous | Step functions | 1D-2D | Hard |
| High-Dim | Lotka-Volterra | 4D-6D | Very Hard |

### Comparison Matrix

For each problem, compare:

|  | Fixed Degree | Fixed Subdivision | Adaptive Heuristic | ML Agent |
|--|--------------|-------------------|-------------------|----------|
| Time to target accuracy | ‚è±Ô∏è | ‚è±Ô∏è | ‚è±Ô∏è | ‚è±Ô∏è |
| Final L2 error | üìâ | üìâ | üìâ | üìâ |
| Function evaluations | üî¢ | üî¢ | üî¢ | üî¢ |
| Critical points found | üéØ | üéØ | üéØ | üéØ |
| Robustness (std over runs) | üìä | üìä | üìä | üìä |

---

## üéâ Long-term Vision

**Year 1**: Proof-of-concept demonstrating ML agents can match/exceed hand-crafted heuristics on toy problems

**Year 2**: Production-ready ML-guided GlobTim with pre-trained models for common problem classes

**Year 3**: Self-improving system that learns from all users' experiments (federated learning), continuously getting better

**Moonshot**: Fully automated "black-box" solver where users specify only function and target accuracy, ML handles all configuration decisions

---

## üìÑ Summary

This milestone proposes a research-driven exploration of machine learning integration with GlobTim's polynomial approximation and optimization framework. By treating approximation decisions (refinement, degree selection, domain decomposition) as learnable policies, we can potentially discover strategies superior to hand-crafted heuristics while reducing the need for expert tuning.

The four research directions offer complementary perspectives:
1. **AMR with RL**: Learn spatial refinement patterns
2. **Neural Controllers**: Apply GlobTim to learned dynamical system controllers
3. **Meta-Learning**: Fast adaptation to new problem types
4. **Degree Adaptation**: Learn optimal polynomial degree progression

Starting with concrete 2D toy examples ensures rapid iteration and clear validation. The Julia ML ecosystem (Flux, ReinforcementLearning.jl, DiffEqFlux) provides excellent tooling. Integration with existing GlobTim infrastructure (parameter tracking, HPC, visualization) enables seamless deployment.

**Key Risk**: ML may not outperform well-tuned heuristics on all problems
**Mitigation**: Focus on problem classes where adaptation is crucial, maintain fallback strategies

**Expected Impact**:
- 20-40% efficiency gains on adaptive-amenable problems
- Reduced need for manual tuning
- New insights into approximation quality landscapes
- Publications at intersection of numerical analysis and ML

---

*This milestone document is a living roadmap. As experiments progress and results emerge, we will iteratively refine the approaches, update success metrics, and adjust timelines. The goal is rigorous scientific exploration, not blind pursuit of ML for its own sake.*
