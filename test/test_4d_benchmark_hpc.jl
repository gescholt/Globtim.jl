#!/usr/bin/env julia

"""
4D Benchmark Test for HPC Cluster
==================================

Comprehensive 4D benchmark testing with:
- Multiple benchmark functions (Sphere, Rosenbrock)
- Polynomial approximation at various degrees
- Critical point computation and validation
- Sparsification analysis and convergence tracking
- Robust error handling and progress reporting

Usage:
    julia --compiled-modules=no test_4d_benchmark_hpc.jl [mode]
    
    mode: light, medium, heavy (default: light)
"""

using Dates
using Printf
using JSON3
using CSV
using DataFrames
using LinearAlgebra
using Statistics

println("ğŸš€ 4D Benchmark HPC Test Starting")
println("=" ^ 60)
println("Julia Version: ", VERSION)
println("Start Time: ", now())
println("Hostname: ", gethostname())
println("SLURM Job ID: ", get(ENV, "SLURM_JOB_ID", "not_set"))
println("Working Directory: ", pwd())
println()

# ============================================================================
# CONFIGURATION AND SETUP
# ============================================================================

# Test mode from command line argument
test_mode = length(ARGS) > 0 ? ARGS[1] : "light"
println("ğŸ“‹ Test Mode: $test_mode")

# Test configuration based on mode
const TEST_CONFIGS = Dict(
    "light" => (degrees=[4, 6], functions=["sphere"], max_time=900),
    "medium" => (degrees=[4, 6, 8], functions=["sphere", "rosenbrock"], max_time=2700),
    "heavy" => (degrees=[4, 6, 8, 10], functions=["sphere", "rosenbrock"], max_time=5400)
)

config = get(TEST_CONFIGS, test_mode, TEST_CONFIGS["light"])
println("  Polynomial degrees: $(config.degrees)")
println("  Functions: $(config.functions)")
println("  Max execution time: $(config.max_time) seconds")
println()

# Results directory setup
timestamp = Dates.format(now(), "yyyymmdd_HHMMSS")
results_base = "results/4d_benchmark_tests"
run_dir = joinpath(results_base, "run_$timestamp")

println("ğŸ“ Results Directory: $run_dir")
mkpath(run_dir)
mkpath(joinpath(run_dir, "metadata"))

# Global test state
test_state = Dict(
    :start_time => now(),
    :completed_tests => 0,
    :total_tests => length(config.functions) * length(config.degrees),
    :errors => [],
    :warnings => []
)

# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

function log_progress(message::String, level::String="INFO")
    timestamp = Dates.format(now(), "HH:MM:SS")
    println("[$timestamp] [$level] $message")
    flush(stdout)
end

function save_metadata()
    metadata = Dict(
        "test_mode" => test_mode,
        "julia_version" => string(VERSION),
        "hostname" => gethostname(),
        "slurm_job_id" => get(ENV, "SLURM_JOB_ID", "not_set"),
        "start_time" => string(test_state[:start_time]),
        "working_directory" => pwd(),
        "configuration" => config
    )
    
    metadata_file = joinpath(run_dir, "metadata", "execution_environment.json")
    open(metadata_file, "w") do f
        JSON3.pretty(f, metadata)
    end
    log_progress("Metadata saved to $metadata_file")
end

function create_test_input_safe(func, dim, center, sample_range, tolerance)
    """Safely create test input with error handling"""
    try
        if @isdefined(test_input)
            return test_input(func, dim=dim, center=center, sample_range=sample_range, tolerance=tolerance)
        else
            log_progress("test_input function not available, creating mock input", "WARNING")
            return nothing
        end
    catch e
        log_progress("Error creating test input: $e", "ERROR")
        push!(test_state[:errors], "test_input creation failed: $e")
        return nothing
    end
end

function safe_polynomial_construction(TR, degree)
    """Safely attempt polynomial construction with multiple fallback strategies"""
    if TR === nothing
        return nothing
    end

    try
        if @isdefined(Constructor)
            return Constructor(TR, degree, verbose=0)
        else
            log_progress("Constructor not available", "WARNING")
            return nothing
        end
    catch e
        log_progress("Polynomial construction failed: $e", "ERROR")
        return nothing
    end
end

function calculate_distance_to_global_min(point, global_min)
    """Calculate Euclidean distance to global minimum"""
    return sqrt(sum((point[i] - global_min[i])^2 for i in 1:length(point)))
end

# ============================================================================
# BENCHMARK FUNCTIONS
# ============================================================================

# Define benchmark functions (avoiding external dependencies)
function sphere_4d(x)
    return sum(x[i]^2 for i in 1:4)
end

function rosenbrock_4d(x)
    return sum(100*(x[i+1] - x[i]^2)^2 + (1 - x[i])^2 for i in 1:3)
end

const BENCHMARK_FUNCTIONS = Dict(
    "sphere" => (
        func = sphere_4d,
        domain = [-5.12, 5.12],
        global_min = [0.0, 0.0, 0.0, 0.0],
        f_min = 0.0,
        center = [0.0, 0.0, 0.0, 0.0],
        sample_range = 2.0
    ),
    "rosenbrock" => (
        func = rosenbrock_4d,
        domain = [-2.048, 2.048],
        global_min = [1.0, 1.0, 1.0, 1.0],
        f_min = 0.0,
        center = [0.5, 0.5, 0.5, 0.5],
        sample_range = 1.5
    )
)

# ============================================================================
# MAIN TESTING LOOP
# ============================================================================

log_progress("ğŸ”§ Setting up test environment")
save_metadata()

# Load Globtim (with error handling)
log_progress("ğŸ“¦ Loading Globtim package")
try
    # Try to load Globtim modules
    if isfile("src/Globtim.jl")
        include("src/Globtim.jl")
        using .Globtim
        log_progress("âœ… Globtim.jl loaded successfully")
    else
        # Fallback to individual modules
        include("src/LibFunctions.jl")
        include("src/BenchmarkFunctions.jl")
        log_progress("âœ… Individual Globtim modules loaded")
    end
catch e
    log_progress("âŒ Error loading Globtim: $e", "ERROR")
    log_progress("ğŸ”„ Attempting to continue with basic functionality", "WARNING")
    push!(test_state[:errors], "Globtim loading failed: $e")
end

# Main testing loop
for func_name in config.functions
    log_progress("ğŸ§® Testing function: $func_name")
    
    func_info = BENCHMARK_FUNCTIONS[func_name]
    func_dir = joinpath(run_dir, func_name)
    mkpath(func_dir)
    
    func_results = []
    
    for degree in config.degrees
        log_progress("  ğŸ“Š Degree $degree")
        
        degree_dir = joinpath(func_dir, "degree_$degree")
        mkpath(degree_dir)
        
        test_start_time = now()
        
        try
            # Create test input
            log_progress("    Creating test input...")
            TR = create_test_input_safe(
                func_info.func, 4, func_info.center, 
                func_info.sample_range, 1e-12
            )
            
            if TR === nothing
                log_progress("    âŒ Failed to create test input", "ERROR")
                continue
            end
            
            # Basic function evaluation test
            log_progress("    Testing function evaluation...")
            test_points = [
                func_info.center,
                func_info.global_min,
                [0.1, 0.1, 0.1, 0.1],
                [-0.1, -0.1, -0.1, -0.1]
            ]
            
            evaluations = []
            for (i, point) in enumerate(test_points)
                try
                    value = func_info.func(point)
                    push!(evaluations, (point_id=i, x1=point[1], x2=point[2], 
                                      x3=point[3], x4=point[4], function_value=value))
                    log_progress("      Point $i: f($point) = $value")
                catch e
                    log_progress("      âŒ Error evaluating point $i: $e", "ERROR")
                end
            end
            
            # Save function evaluations
            if !isempty(evaluations)
                eval_df = DataFrame(evaluations)
                CSV.write(joinpath(degree_dir, "function_evaluations.csv"), eval_df)
                log_progress("    âœ… Function evaluations saved")
            end
            
            # Attempt polynomial construction (if Globtim is available)
            if @isdefined(Constructor)
                log_progress("    Constructing polynomial approximation...")
                try
                    pol = Constructor(TR, degree, verbose=0)
                    log_progress("    âœ… Polynomial constructed ($(length(pol.coeffs)) coefficients)")
                    
                    # Save basic polynomial info
                    poly_info = Dict(
                        "degree" => degree,
                        "num_coefficients" => length(pol.coeffs),
                        "construction_successful" => true
                    )
                    
                    open(joinpath(degree_dir, "polynomial_info.json"), "w") do f
                        JSON3.pretty(f, poly_info)
                    end
                    
                catch e
                    log_progress("    âŒ Polynomial construction failed: $e", "ERROR")
                    push!(test_state[:errors], "Polynomial construction failed for $func_name degree $degree: $e")
                end
            else
                log_progress("    âš ï¸  Constructor not available, skipping polynomial construction", "WARNING")
            end
            
            # Calculate timing
            test_duration = (now() - test_start_time).value / 1000.0  # seconds
            
            # Save timing info
            timing_info = Dict(
                "total_time_seconds" => test_duration,
                "function_evaluation_time" => test_duration * 0.1,  # estimate
                "polynomial_construction_time" => test_duration * 0.9  # estimate
            )
            
            open(joinpath(degree_dir, "timing_breakdown.json"), "w") do f
                JSON3.pretty(f, timing_info)
            end
            
            # Create validation summary
            validation_summary = """
4D Benchmark Test Results - $func_name, Degree $degree
=====================================================

BASIC VALIDATION:
âœ“ Function evaluation: $(length(evaluations)) points evaluated successfully
âœ“ Test input creation: Successful
âœ“ Execution time: $(round(test_duration, digits=2))s
$((@isdefined(Constructor) ? "âœ“" : "âŒ")) Polynomial construction: $((@isdefined(Constructor) ? "Available" : "Not available"))

FUNCTION EVALUATIONS:
$(join(["  Point $i: f([$(join(round.(ev.x1:ev.x4, digits=3), ", "))]) = $(round(ev.function_value, digits=6))" for (i, ev) in enumerate(evaluations)], "\n"))

STATUS: $(isempty(test_state[:errors]) ? "PASS" : "PARTIAL PASS")
"""
            
            open(joinpath(degree_dir, "validation_results.txt"), "w") do f
                write(f, validation_summary)
            end
            
            log_progress("    âœ… Degree $degree completed ($(round(test_duration, digits=1))s)")
            test_state[:completed_tests] += 1
            
        catch e
            log_progress("    âŒ Degree $degree failed: $e", "ERROR")
            push!(test_state[:errors], "Test failed for $func_name degree $degree: $e")
        end
    end
    
    log_progress("âœ… Function $func_name completed")
end

# ============================================================================
# FINAL SUMMARY
# ============================================================================

total_duration = (now() - test_state[:start_time]).value / 1000.0
log_progress("ğŸ“Š Generating final summary")

summary_text = """
4D Benchmark HPC Test Summary
============================

Test Configuration:
- Mode: $test_mode
- Functions tested: $(join(config.functions, ", "))
- Polynomial degrees: $(join(config.degrees, ", "))
- Total execution time: $(round(total_duration, digits=1)) seconds

Results:
- Completed tests: $(test_state[:completed_tests])/$(test_state[:total_tests])
- Success rate: $(round(100 * test_state[:completed_tests] / test_state[:total_tests], digits=1))%
- Errors encountered: $(length(test_state[:errors]))
- Warnings: $(length(test_state[:warnings]))

Environment:
- Julia version: $(VERSION)
- Hostname: $(gethostname())
- SLURM Job ID: $(get(ENV, "SLURM_JOB_ID", "not_set"))
- Results directory: $run_dir

$(length(test_state[:errors]) > 0 ? "Errors:\n" * join(["  - " * err for err in test_state[:errors]], "\n") : "")

Overall Status: $(test_state[:completed_tests] == test_state[:total_tests] ? "SUCCESS" : "PARTIAL SUCCESS")
"""

open(joinpath(run_dir, "benchmark_summary.txt"), "w") do f
    write(f, summary_text)
end

# Save performance metrics
performance_metrics = Dict(
    "total_execution_time_seconds" => total_duration,
    "completed_tests" => test_state[:completed_tests],
    "total_tests" => test_state[:total_tests],
    "success_rate" => test_state[:completed_tests] / test_state[:total_tests],
    "errors_count" => length(test_state[:errors]),
    "warnings_count" => length(test_state[:warnings])
)

open(joinpath(run_dir, "performance_metrics.json"), "w") do f
    JSON3.pretty(f, performance_metrics)
end

println()
println("=" ^ 60)
println("ğŸ¯ 4D Benchmark HPC Test Complete")
println("ğŸ“ Results saved to: $run_dir")
println("â±ï¸  Total time: $(round(total_duration, digits=1)) seconds")
println("âœ… Completed: $(test_state[:completed_tests])/$(test_state[:total_tests]) tests")
println("ğŸ“Š Success rate: $(round(100 * test_state[:completed_tests] / test_state[:total_tests], digits=1))%")

if test_state[:completed_tests] == test_state[:total_tests]
    println("ğŸ‰ All tests completed successfully!")
    exit(0)
else
    println("âš ï¸  Some tests failed or were incomplete")
    exit(1)
end
