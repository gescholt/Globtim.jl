{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to generate three graphs: \n",
    "- Histogram of Number of `loc_min` points were found, (so outputs of the Optim routine) and what percentage of them is within a small distance of a critical point of the approximant. As a function of the degree `d` of the approximant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "using Revise\n",
    "Pkg.activate(\"../../.\")\n",
    "using Globtim\n",
    "using DynamicPolynomials, DataFrames\n",
    "using ProgressLogging\n",
    "using Optim\n",
    "using CairoMakie\n",
    "using LinearAlgebra\n",
    "using CSV  # Add CSV here for loading reference data\n",
    "CairoMakie.activate!()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_min, d_max = 2, 10  # Using max degree 4 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the legacy approach with load_function_params\n",
    "# Deuflhard_4d is already defined in Globtim\n",
    "params = load_function_params(\"Deuflhard_4d\")\n",
    "TR = test_input(Deuflhard_4d;\n",
    "    dim=params.dim,\n",
    "    center=params.center,\n",
    "    GN=params.num_samples,\n",
    "    sample_range=params.sample_range,\n",
    "    tolerance=params.tolerance)\n",
    "\n",
    "@polyvar(x[1:TR.dim]); # Define polynomial ring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-computed 4D critical points from the orthant analysis\n",
    "df_orthant_critical = CSV.read(\"../ForwardDiff_Certification/by_degree/data/4d_all_critical_points_orthant.csv\", DataFrame)\n",
    "\n",
    "# Extract just the coordinates and function values\n",
    "df_reference_points = DataFrame(\n",
    "    x1 = df_orthant_critical.x1,\n",
    "    x2 = df_orthant_critical.x2, \n",
    "    x3 = df_orthant_critical.x3,\n",
    "    x4 = df_orthant_critical.x4,\n",
    "    z = df_orthant_critical.function_value\n",
    ")\n",
    "\n",
    "println(\"Loaded $(nrow(df_reference_points)) reference critical points from orthant analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_reference_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze polynomial approximations for different degrees\n",
    "results = Dict{Int,Any}()\n",
    "\n",
    "# Make sure we have reference points loaded\n",
    "if !@isdefined(df_reference_points)\n",
    "    error(\"Please run the cell that loads df_reference_points first!\")\n",
    "end\n",
    "\n",
    "for d in d_min:d_max\n",
    "    println(\"Analyzing degree $d...\")\n",
    "    \n",
    "    # Construct polynomial for this degree\n",
    "    pol = Constructor(TR, d, basis=:chebyshev, verbose=false)\n",
    "    \n",
    "    # Extract actual degree (handle both Int and Tuple cases)\n",
    "    actual_degree = pol.degree isa Tuple ? pol.degree[2] : pol.degree\n",
    "    \n",
    "    # Solve polynomial system\n",
    "    solutions = solve_polynomial_system(x, 4, actual_degree, pol.coeffs, basis=:chebyshev)\n",
    "    \n",
    "    # Process critical points\n",
    "    df_crit = process_crit_pts(solutions, Deuflhard_4d, TR)\n",
    "    \n",
    "    # Apply BFGS refinement to critical points\n",
    "    refined_points = []\n",
    "    captured_flags = Bool[]\n",
    "    \n",
    "    for i in 1:nrow(df_crit)\n",
    "        pt = [df_crit[i, Symbol(\"x$j\")] for j in 1:4]\n",
    "        res = Optim.optimize(Deuflhard_4d, pt, BFGS(), \n",
    "                           Optim.Options(g_tol=1e-8, f_abstol=1e-20, x_abstol=1e-12))\n",
    "        \n",
    "        refined_pt = Optim.minimizer(res)\n",
    "        push!(refined_points, refined_pt)\n",
    "        \n",
    "        # Check if point is captured by comparing to reference critical points\n",
    "        min_dist_to_ref = Inf\n",
    "        for j in 1:nrow(df_reference_points)\n",
    "            ref_pt = [df_reference_points[j, Symbol(\"x$k\")] for k in 1:4]\n",
    "            dist = norm(refined_pt - ref_pt)\n",
    "            min_dist_to_ref = min(min_dist_to_ref, dist)\n",
    "        end\n",
    "        \n",
    "        # Point is captured if it's close to a reference critical point\n",
    "        push!(captured_flags, min_dist_to_ref < 0.001)  # Using tolerance of 0.001\n",
    "    end\n",
    "    \n",
    "    # Create dataframe with refined points\n",
    "    df_min = DataFrame()\n",
    "    for j in 1:4\n",
    "        df_min[!, Symbol(\"x$j\")] = [p[j] for p in refined_points]\n",
    "    end\n",
    "    df_min[!, :z] = [Deuflhard_4d(p) for p in refined_points]\n",
    "    df_min[!, :captured] = captured_flags\n",
    "    \n",
    "    # Store results in format expected by notebook functions\n",
    "    results[d] = (\n",
    "        df = df_crit,        # Critical points from polynomial\n",
    "        df_min = df_min,     # Refined points with captured flag\n",
    "        convergence_stats = (nrm = pol.nrm,),\n",
    "        discrete_l2 = pol.nrm\n",
    "    )\n",
    "    \n",
    "    println(\"  Found $(nrow(df_crit)) critical points, $(sum(captured_flags)) captured (using orthant reference)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the extension module directly\n",
    "const GlobtimExt = Base.get_extension(Globtim, :GlobtimCairoMakieExt)\n",
    "const plot_discrete_l2 = GlobtimExt.plot_discrete_l2\n",
    "const capture_histogram = GlobtimExt.capture_histogram\n",
    "const plot_convergence_analysis = GlobtimExt.plot_convergence_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the tensored 4d dataframe and load CSV data\n",
    "using IterTools\n",
    "using CSV\n",
    "\n",
    "function double_dataframe(df::DataFrame)\n",
    "    n = nrow(df)\n",
    "    pairs = collect(product(1:n, 1:n))\n",
    "\n",
    "    x1 = vec([df.x[j[1]] for j in pairs])\n",
    "    y1 = vec([df.y[j[1]] for j in pairs])\n",
    "    x2 = vec([df.x[j[2]] for j in pairs])\n",
    "    y2 = vec([df.y[j[2]] for j in pairs])\n",
    "\n",
    "    return DataFrame(x1=x1, x2=y1, x3=x2, x4=y2)\n",
    "end\n",
    "\n",
    "df_chebfun = CSV.read(\"../../data/matlab_critical_points/valid_points_deuflhard.csv\", DataFrame)\n",
    "df_4d = double_dataframe(df_chebfun);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_1 = plot_discrete_l2(results, d_min, d_max, 1)\n",
    "save(\"discrete_l2.pdf\", fig_1)\n",
    "display(fig_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_2 = capture_histogram(results, d_min, d_max, 1, tol_dist=.1, show_legend=false)\n",
    "save(\"histogram.pdf\", fig_2)\n",
    "display(fig_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add histogram_enhanced to the imported functions\n",
    "const histogram_enhanced = GlobtimExt.histogram_enhanced\n",
    "\n",
    "# Create enhanced histogram using orthant critical points\n",
    "fig_enhanced = histogram_enhanced(\n",
    "    results, \n",
    "    df_orthant_critical,  # Full theoretical critical points data\n",
    "    d_min, \n",
    "    d_max, \n",
    "    1,\n",
    "    tol_bfgs = 0.001,    # Tolerance for BFGS convergence\n",
    "    tol_raw = 0.1,       # Tolerance for raw points  \n",
    "    show_legend = true\n",
    ")\n",
    "save(\"histogram_enhanced.pdf\", fig_enhanced)\n",
    "display(fig_enhanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram Comparison\n",
    "\n",
    "The **original histogram** (above) shows captured vs uncaptured based on whether BFGS refinement moved the point by less than 0.1.\n",
    "\n",
    "The **enhanced histogram** (above) specifically tracks convergence to the 9 theoretical minimizers:\n",
    "- **Blue bars**: BFGS points that converged to theoretical minimizers (tolerance = 0.001)  \n",
    "- **Green bars**: Raw polynomial critical points already close to theoretical minimizers (tolerance = 0.1)\n",
    "\n",
    "This enhanced view shows:\n",
    "1. How many of the polynomial's critical points actually lead to true minima after refinement\n",
    "2. How many raw critical points are already good approximations of minima (green)\n",
    "3. The polynomial degree needed to capture all 9 theoretical minimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function expected by plot_convergence_analysis\n",
    "function analyze_convergence_distances(df::DataFrame)\n",
    "    # For each critical point, compute distance to nearest other critical point\n",
    "    n_points = nrow(df)\n",
    "    x_cols = [col for col in names(df) if startswith(string(col), \"x\")]\n",
    "    n_dims = length(x_cols)\n",
    "    \n",
    "    min_distances = Float64[]\n",
    "    \n",
    "    for i in 1:n_points\n",
    "        point1 = [df[i, Symbol(\"x$j\")] for j in 1:n_dims]\n",
    "        min_dist = Inf\n",
    "        \n",
    "        for j in 1:n_points\n",
    "            if i != j\n",
    "                point2 = [df[j, Symbol(\"x$j\")] for j in 1:n_dims]\n",
    "                dist = norm(point1 - point2)\n",
    "                min_dist = min(min_dist, dist)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if isfinite(min_dist)\n",
    "            push!(min_distances, min_dist)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if isempty(min_distances)\n",
    "        return (maximum=0.0, average=0.0)\n",
    "    end\n",
    "    \n",
    "    return (\n",
    "        maximum=maximum(min_distances),\n",
    "        average=sum(min_distances) / length(min_distances)\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_3 = plot_convergence_analysis(results, d_min, d_max, 1, show_legend=true)\n",
    "# save(\"convergence_analysis.pdf\", fig_3)\n",
    "display(fig_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add histogram_minimizers_only to the imported functions\n",
    "const histogram_minimizers_only = GlobtimExt.histogram_minimizers_only\n",
    "\n",
    "# Create minimizers-only histogram\n",
    "fig_minima = histogram_minimizers_only(\n",
    "    results, \n",
    "    df_orthant_critical,\n",
    "    d_min, \n",
    "    d_max, \n",
    "    1,\n",
    "    tol_theoretical = 0.001,   # Tolerance for matching theoretical points\n",
    "    show_legend = false\n",
    ")\n",
    "save(\"histogram_minima_only.pdf\", fig_minima)\n",
    "display(fig_minima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizers-Only Histogram\n",
    "\n",
    "This histogram focuses exclusively on **minimum points** (where f < 1e-6), filtering out saddle points and maxima:\n",
    "\n",
    "- **Blue bars**: BFGS refined points that are minima (low function value)\n",
    "- **Green bars**: Raw polynomial critical points that are already minima  \n",
    "- **Dark blue bars**: BFGS minima that match theoretical minimizers\n",
    "- **Red dashed line**: The 9 theoretical minimizers we're trying to find\n",
    "\n",
    "This view shows:\n",
    "1. Not all polynomial critical points are minima (compare total points vs minima)\n",
    "2. Some raw polynomial points are already good minima approximations (green)\n",
    "3. Progress toward finding all 9 theoretical minimizers as degree increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "function compute_min_distances(df, df_check)\n",
    "    # Initialize array to store minimum distances\n",
    "    min_distances = Float64[]\n",
    "\n",
    "    # For each row in df, find distance to closest point in df_check\n",
    "    for i in 1:nrow(df)\n",
    "        point = [df[i, Symbol(\"x$j\")] for j in 1:4]  # Extract point coordinates\n",
    "        min_dist = Inf\n",
    "\n",
    "        # Compare with each point in df_check\n",
    "        for j in 1:nrow(df_check)\n",
    "            check_point = [df_check[j, Symbol(\"x$j\")] for j in 1:4]\n",
    "            dist = norm(point - check_point)  # Euclidean distance\n",
    "            min_dist = min(min_dist, dist)\n",
    "        end\n",
    "\n",
    "        push!(min_distances, min_dist)\n",
    "    end\n",
    "\n",
    "    return min_distances\n",
    "end\n",
    "\n",
    "function analyze_captured_distances(df, df_check)\n",
    "    distances = compute_min_distances(df, df_check)\n",
    "    return (\n",
    "        maximum=maximum(distances),\n",
    "        average=sum(distances) / length(distances)\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the plot_convergence_captured function\n",
    "function plot_convergence_captured(results, df_check, start_degree::Int, end_degree::Int, step::Int; show_legend::Bool=true)\n",
    "    degrees = start_degree:step:end_degree\n",
    "    max_distances = Float64[]\n",
    "    avg_distances = Float64[]\n",
    "\n",
    "    for d in degrees\n",
    "        x_cols = [col for col in names(results[d].df_min) if startswith(string(col), \"x\")]\n",
    "        df = results[d].df_min[:, x_cols]\n",
    "\n",
    "        stats = analyze_captured_distances(df, df_check)\n",
    "        push!(max_distances, stats.maximum)\n",
    "        push!(avg_distances, stats.average)\n",
    "    end\n",
    "\n",
    "    fig = Figure(size=(600, 400))\n",
    "\n",
    "    ax = Axis(fig[1, 1],\n",
    "        # xlabel removed per user request\n",
    "        # ylabel removed per user request\n",
    "        )\n",
    "\n",
    "    scatterlines!(ax, degrees, max_distances, label=\"Maximum\", color=:red)\n",
    "    scatterlines!(ax, degrees, avg_distances, label=\"Average\", color=:blue)\n",
    "\n",
    "    # Legend removed per user request - respect show_legend parameter\n",
    "    # if show_legend\n",
    "    #     axislegend(ax)\n",
    "    # end\n",
    "\n",
    "    return fig\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "# Use the reference critical points from orthant analysis instead of df_4d\n",
    "fig_4 = plot_convergence_captured(results, df_reference_points, d_min, d_max, 1, show_legend=false)\n",
    "save(\"convergence_captured_orthant.pdf\", fig_4)\n",
    "display(fig_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of capture rates with orthant reference points\n",
    "println(\"\\n=== Capture Rate Summary (using orthant reference points) ===\")\n",
    "println(\"Degree | Total Points | Captured | Uncaptured | Capture Rate\")\n",
    "println(\"-------|--------------|----------|------------|-------------\")\n",
    "for d in d_min:d_max\n",
    "    total = nrow(results[d].df_min)\n",
    "    captured = sum(results[d].df_min.captured)\n",
    "    uncaptured = total - captured\n",
    "    rate = captured / total * 100\n",
    "    println(\"   $d   |      $(lpad(total, 3))     |    $(lpad(captured, 3))   |     $(lpad(uncaptured, 3))    |   $(round(rate, digits=1))%\")\n",
    "end\n",
    "\n",
    "println(\"\\nNote: Points are considered 'captured' if they are within 0.001 distance\")\n",
    "println(\"of any reference critical point from the orthant analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which reference points are being found\n",
    "println(\"\\n=== Reference Point Coverage Analysis ===\")\n",
    "println(\"Checking which of the $(nrow(df_reference_points)) reference points are found by degree 8...\")\n",
    "\n",
    "# For degree 8, check which reference points are matched\n",
    "d8_refined = results[8].df_min\n",
    "ref_found = falses(nrow(df_reference_points))\n",
    "\n",
    "for i in 1:nrow(df_reference_points)\n",
    "    ref_pt = [df_reference_points[i, Symbol(\"x$j\")] for j in 1:4]\n",
    "    \n",
    "    # Check if any refined point is close to this reference point\n",
    "    for j in 1:nrow(d8_refined)\n",
    "        refined_pt = [d8_refined[j, Symbol(\"x$k\")] for k in 1:4]\n",
    "        dist = norm(refined_pt - ref_pt)\n",
    "        if dist < 0.001\n",
    "            ref_found[i] = true\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"Reference points found: $(sum(ref_found)) out of $(nrow(df_reference_points))\")\n",
    "println(\"Coverage: $(round(sum(ref_found)/nrow(df_reference_points)*100, digits=1))%\")\n",
    "\n",
    "# Show which types of points are missing\n",
    "if \"type_4d\" in names(df_orthant_critical)\n",
    "    println(\"\\nBreakdown by type:\")\n",
    "    for type in unique(df_orthant_critical.type_4d)\n",
    "        type_mask = df_orthant_critical.type_4d .== type\n",
    "        type_found = sum(ref_found[type_mask])\n",
    "        type_total = sum(type_mask)\n",
    "        println(\"  $type: $type_found/$type_total found ($(round(type_found/type_total*100, digits=1))%)\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Four Key Visualizations\n",
    "\n",
    "We've generated four important graphs to analyze the polynomial approximation quality:\n",
    "\n",
    "1. **Discrete L2 Norm** (`discrete_l2.pdf`): Shows how the approximation error decreases as polynomial degree increases\n",
    "\n",
    "2. **Original Histogram** (`histogram.pdf`): Shows captured vs uncaptured points based on BFGS movement distance\n",
    "\n",
    "3. **Enhanced Histogram** (`histogram_enhanced.pdf`): Tracks convergence to the 9 theoretical minimizers:\n",
    "   - Blue bars: BFGS refined points converging to minimizers\n",
    "   - Green bars: Raw polynomial critical points already near minimizers\n",
    "   \n",
    "4. **Minimizers-Only Histogram** (`histogram_minima_only.pdf`): Focuses only on minimum points:\n",
    "   - Shows which critical points are actual minima (f < 1e-6)\n",
    "   - Tracks progress toward finding all 9 theoretical minimizers\n",
    "   \n",
    "5. **Convergence Analysis** (`convergence_captured_orthant.pdf`): Shows maximum and average distances from refined points to the nearest theoretical critical point\n",
    "\n",
    "These visualizations demonstrate that:\n",
    "- Higher degree polynomials better approximate the function (lower L2 norm)\n",
    "- Not all critical points found by the polynomial are true minima\n",
    "- The polynomial approximation progressively captures more of the 9 theoretical minimizers as degree increases\n",
    "- Some polynomial degrees find many critical points but few are actual minima"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
