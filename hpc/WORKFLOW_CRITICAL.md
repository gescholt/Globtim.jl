# üö® CRITICAL: HPC Workflow Guide - READ FIRST üö®

**This is the FIRST document you must read before working with the HPC cluster.**

## üéØ The Golden Rule

**Code Management**: mack (fileserver) ONLY  
**Job Submission**: falcon (cluster) ONLY

## üìã Step-by-Step Workflow

### Step 1: Code Management (via mack)
```bash
# Connect to fileserver for ALL code operations
ssh scholten@mack
cd ~/globtim_hpc

# Upload code files
scp -r local_changes/* scholten@mack:~/globtim_hpc/

# Upload large files (5GB+) - use mack (dedicated export node)
scp large_dataset.h5 scholten@mack:~/globtim_hpc/data/
rsync -avz --progress large_files/ scholten@mack:~/globtim_hpc/data/

# Install Julia packages (302 packages available)
julia --project=. -e 'using Pkg; Pkg.add("NewPackage")'

# Modify code, prepare data, organize files
# ALL file operations must happen here!
```

### Step 2: Job Submission (via falcon)
```bash
# Connect to cluster for job submission ONLY
ssh scholten@falcon
cd ~/globtim_hpc

# Submit SLURM jobs (required parameters)
sbatch --account=mpi --partition=batch your_job.slurm

# Monitor jobs
squeue -u scholten
sacct -j <job_id>
```

### Step 3: Results Collection
```bash
# Results are accessible from both locations
# Via cluster:
ssh scholten@falcon 'ls -la ~/globtim_hpc/results/'

# Via fileserver:
ssh scholten@mack 'ls -la ~/globtim_hpc/results/'

# Download results locally (use mack for large files):
scp -r scholten@mack:~/globtim_hpc/results/experiment_* ./local_results/
rsync -avz --progress scholten@mack:~/globtim_hpc/results/ ./local_results/
```

## ‚ö†Ô∏è Critical Warnings

### NEVER DO THESE:
- ‚ùå Install packages on falcon (1GB quota limit)
- ‚ùå Run jobs from `/tmp` (forbidden)
- ‚ùå Submit jobs from mack (no SLURM scheduler)
- ‚ùå Store large files in falcon home directory
- ‚ùå Modify code directly on falcon

### ALWAYS DO THESE:
- ‚úÖ Upload code via mack
- ‚úÖ Submit jobs via falcon
- ‚úÖ Use `--account=mpi --partition=batch`
- ‚úÖ Work in `~/globtim_hpc` directory
- ‚úÖ Keep falcon home directory clean (only globtim_hpc)

## üîß Architecture Overview

```
Local Development
       ‚Üì (scp/rsync)
mack (fileserver)
  ‚Ä¢ Code storage
  ‚Ä¢ Package management  
  ‚Ä¢ Data preparation
  ‚Ä¢ Results collection
       ‚Üì (NFS mount)
falcon (cluster)
  ‚Ä¢ Job submission
  ‚Ä¢ SLURM scheduler
  ‚Ä¢ Job monitoring
       ‚Üì (SLURM execution)
Compute Nodes
  ‚Ä¢ Job execution
  ‚Ä¢ Access mack via NFS
  ‚Ä¢ Write results to mack
```

## üìä Quota Information

- **falcon home**: 1GB limit (CRITICAL - keep minimal!)
- **mack**: Generous storage for code and results
- **NFS**: Compute nodes access mack storage seamlessly

## üöÄ Quick Commands

```bash
# Check falcon quota
ssh scholten@falcon 'quota -u scholten'

# Check job status
ssh scholten@falcon 'squeue -u scholten'

# View recent job results
ssh scholten@falcon 'sacct -u scholten --starttime=today'

# Clean falcon home (keep only globtim_hpc)
ssh scholten@falcon 'cd ~ && ls -la | grep -v globtim_hpc'

# Upload code to fileserver
scp -r ./src/* scholten@mack:~/globtim_hpc/src/
```

## üìö Related Documentation

- `hpc/README.md` - Detailed HPC infrastructure guide
- `hpc/docs/HPC_STATUS_SUMMARY.md` - Current status and troubleshooting
- `hpc/docs/SLURM_DIAGNOSTIC_GUIDE.md` - SLURM troubleshooting
- `hpc/jobs/submission/` - Job submission scripts

## üÜò Emergency Procedures

### If falcon quota exceeded:
```bash
ssh scholten@falcon 'cd ~ && rm -f *.out *.err *.log *.slurm'
```

### If jobs fail with exit code 0:53:
- Check falcon quota: `quota -u scholten`
- Clean old files from falcon home
- Ensure working in `~/globtim_hpc`
- Verify `--account=mpi` in SLURM script

### If packages missing:
- Install on mack: `ssh scholten@mack`
- Never install on falcon

---

**Remember**: mack for code, falcon for jobs. Follow this rule and everything works smoothly! üéØ
