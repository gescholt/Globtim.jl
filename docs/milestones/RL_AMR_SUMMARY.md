# RL-Guided Adaptive Mesh Refinement - Executive Summary

**Date**: 2025-10-09
**Milestone**: #6
**Status**: Ready for Implementation

---

## ğŸ¯ **What We're Building**

A reinforcement learning agent that learns to **efficiently find all significant local minimizers** of smooth objective functions by adaptively refining polynomial approximation subdomains.

### **Key Design Choices**

1. âœ… **Single Agent**: One policy network evaluates all subdomains (simpler than multi-agent)
2. âœ… **Ground Truth Evaluation**: Use ForwardDiff on true function, not symbolic solving
3. âœ… **Budget-Agnostic**: Reward based on minimizers found, works with any stopping criterion
4. âœ… **Scaling Path**: 2D â†’ 3D â†’ 4D+ (smooth functions with multiple minima)

---

## ğŸ† **The Reward Function**

```julia
# What we reward:
reward = (
    10.0 * num_novel_local_minimizers +    # PRIMARY: Find new minimizers
    1.0  * progress_toward_minimizers +    # SECONDARY: Dense learning signal
    -0.1 * num_new_subdomains             # PENALTY: Efficiency
)

# How we detect local minimizers:
is_local_minimizer = (
    norm(âˆ‡f(x)) < 1e-6 &&                 # Small gradient (critical point)
    all(eigvals(H(x)) .> 0)               # Positive definite Hessian
)
```

**Why no exact solution required?** We directly evaluate the gradient on the **true function** using ForwardDiff during trainingâ€”no need for HomotopyContinuation's symbolic solving.

---

## ğŸ§ª **Test Progression**

### **Phase 1: 1D (Weeks 1-4) - Sanity Checks**
- Single minimum: `f(x) = (x - 0.5)Â²`
- Two minima: `f(x) = (xÂ² - 1)Â²`
- Multiple minima: `f(x) = sin(5Ï€x) + 0.5xÂ²`

**Goal**: Verify basic RL infrastructure works

---

### **Phase 2: 2D (Weeks 5-10) - Core Development**

| Function | Minima | Difficulty | Purpose |
|----------|--------|------------|---------|
| **Rosenbrock** | 1 | Medium | Narrow valley refinement |
| **Six-Hump Camel** | 2 | Medium | Multiple minima discovery |
| **Himmelblau** | 4 | Medium | Completeness (find all 4) |
| **Rastrigin** | Many | Hard | Stress test multimodality |

**Goal**: Demonstrate efficiency gains vs baselines

---

### **Phase 3: 3D+ (Weeks 11-16) - Scaling**
- 3D Rosenbrock
- 4D Styblinski-Tang

**Goal**: Test transfer learning and dimensionality scaling

---

## ğŸ“Š **Success Criteria**

We beat baselines if:
- **Completeness**: Find â‰¥90% of true minimizers
- **Precision**: â‰¥90% of reported points are true minimizers
- **Efficiency**: Use â‰¤70% computational cost vs uniform refinement

---

## ğŸ“¦ **Julia Packages**

```julia
# Core RL
using ReinforcementLearning      # PPO algorithm
using Lux                         # Neural network (policy)
using CommonRLInterface          # Environment standard

# Math & Evaluation
using ForwardDiff                # Gradients & Hessians
using LinearAlgebra              # Eigenvalues

# Existing GlobTim
using Globtim                    # Polynomial approximation
```

---

## ğŸ› ï¸ **Implementation Steps**

### **Week 1-2: Infrastructure**
```julia
# 1. Data structures
struct GlobTimState
    subdomains::Vector{Subdomain}
    known_minimizers::Vector{Vector{Float64}}
end

# 2. RL Environment
struct GlobTimAMREnv <: CommonRLInterface.AbstractEnv
    objective_func::Function
    state::Ref{GlobTimState}
end

# 3. Reward function
function compute_reward(f, state_before, actions, state_after)
    # Count novel local minimizers using ForwardDiff
end
```

### **Week 3-4: 1D Validation**
```julia
# Train simple agent
agent = PPOAgent(policy_network)
train!(agent, env_1d, n_episodes=100)

# Verify it finds minima faster than uniform
```

### **Week 5-10: 2D Benchmarks**
```julia
# Train on Rosenbrock, Six-Hump, Himmelblau, Rastrigin
# Compare vs 3 baselines
# Generate performance plots
```

---

## ğŸ”¬ **Baselines for Comparison**

1. **Uniform**: Subdivide all subdomains equally
2. **Error-Greedy**: Refine highest L2 approximation error
3. **Gradient-Heuristic**: Refine highest gradient magnitude

---

## ğŸŒŸ **Why This Will Work**

### **Precedent**
- RL for AMR in finite elements (Luca et al. 2024) showed 30% efficiency gains
- Active learning for sample placement well-established
- Julia ML ecosystem mature (ReinforcementLearning.jl, Lux.jl)

### **Advantages Over Heuristics**
- **Learns patterns**: E.g., "narrow valleys need fine refinement"
- **Problem-specific**: Adapts to function characteristics
- **End-to-end**: Optimizes for final goal (finding minima), not proxy (L2 error)

### **Risk Mitigation**
- Start simple (1D, 2D) before scaling
- Dense reward signal (progress toward minima)
- Multiple baselines for comparison
- Budget-agnostic design allows flexible stopping criteria

---

## ğŸ“ˆ **Expected Outcomes**

### **Short-term (3 months)**
- âœ… Working prototype on 2D benchmarks
- âœ… 20-40% efficiency gains vs uniform refinement
- âœ… Documentation of reward design lessons

### **Medium-term (6 months)**
- âœ… Scaling to 3D-4D problems
- âœ… Transfer learning experiments (2D â†’ higher-D)
- âœ… First research paper draft

### **Long-term Vision**
- ğŸ¯ Production AMR agent in GlobTim
- ğŸ¯ Pre-trained models for common problem classes
- ğŸ¯ "Black-box" mode: user provides function â†’ agent finds all minima

---

## ğŸš¨ **Key Challenges**

| Challenge | Mitigation |
|-----------|-----------|
| Sparse rewards initially | Dense progress signal + curriculum learning |
| Hessian computation cost | Pre-filter with gradient, cache evaluations |
| Dimensionality scaling | Dimension-agnostic features, transfer learning |
| Defining "significant" basin | Make threshold hyperparameter, compare variants |

---

## ğŸ“š **Related Documents**

- **Full milestone**: [MACHINE_LEARNING_INTEGRATION.md](MACHINE_LEARNING_INTEGRATION.md)
- **Detailed plan**: [MILESTONE_6_DETAILED_IMPLEMENTATION.md](MILESTONE_6_DETAILED_IMPLEMENTATION.md)
- **Package research**: See detailed implementation doc for package survey

---

## ğŸš€ **Next Actions**

1. **This week**: Implement `GlobTimState` and reward function
2. **Next week**: Setup RL environment with CommonRLInterface
3. **Week 3**: First training run on 1D single minimum
4. **Week 4**: Debug and validate 1D results
5. **Week 5**: Scale to 2D Rosenbrock

---

## ğŸ’¡ **The Big Insight**

Traditional AMR uses hand-crafted heuristics (refine where error is high). But the **true goal** is finding critical points, not minimizing approximation error. RL allows us to **directly optimize for the end goal**.

```julia
# Traditional heuristic:
refine_where = argmax(l2_errors)  # Proxy metric

# RL approach:
refine_where = policy(state)  # Learned to maximize minimizer discovery
```

This is the power of end-to-end learning! ğŸ¯

---

*For questions, see detailed documentation or reach out to the GlobTim team.*
